# =============================================================================
# BrikPipe â€” Containerized Integration Tests
# -----------------------------------------------------------------------------
# PIPE-FLAKY-POLICY-CONFIG-002 wiring added:
#   - Select policy (repo override -> central fallback)
#   - Normalize flaky summary to satisfy evaluator contract (prevents total=0)
#   - Evaluate against policy with optional enforcement
#   - Upload flaky-eval artifact per shard
#
# Extra hardening:
#   - Setup Node (20) explicitly for evaluator stability
#   - Ensures flaky wrapper evidence is normalized BEFORE evaluation
#   - Runs evaluator from repo-root with local js-yaml + tsx installed
#   - Doesn't break if evidence missing (e.g. wrapper never ran due to earlier failure)
# =============================================================================
name: "BrikPipe â€” Containerized Integration Tests"

on:
  workflow_call:
    inputs:
      image:
        description: >
          Full tag for the service image under test. If the image does not
          exist in the registry yet, this workflow will attempt to pull it
          and, on failure, build it locally using dockerfile + context.
        required: true
        type: string

      dockerfile:
        description: "Path to the Dockerfile for building the service image under test."
        required: false
        type: string
        default: "./Dockerfile"

      context:
        description: "Build context directory for the service image under test."
        required: false
        type: string
        default: "."

      db_image:
        description: "Database image to use for integration tests. Example: postgres:16."
        required: false
        type: string
        default: "postgres:16"

      cache_image:
        description: >
          Optional cache or message broker image (e.g., redis:7,
          rabbitmq:3-management). Used only when enable_cache_service is true.
        required: false
        type: string
        default: ""

      enable_cache_service:
        description: >
          If true, start a cache/message broker container using cache_image.
        required: false
        type: boolean
        default: false

      test_language:
        description: >
          Language hint for the test runner. Supported: node | python | java
          | go | dotnet. Used when test_command is empty.
        required: false
        type: string
        default: ""

      test_command:
        description: >
          Shell command to run inside the test-runner container to execute
          integration tests. If empty, the runner falls back to language-
          specific defaults based on test_language.
        required: false
        type: string
        default: ""

      healthcheck_path:
        description: >
          Path on the app for readiness probing (combined with app_port).
          Example: /health or /actuator/health.
        required: false
        type: string
        default: "/health"

      healthcheck_timeout:
        description: "Maximum number of seconds to wait for DB + app readiness."
        required: false
        type: string
        default: "60"

      app_port:
        description: "Port exposed by the app container for HTTP health checks."
        required: false
        type: string
        default: "3000"

      db_port:
        description: >
          Host port to expose Postgres on (diagnostics/seeding from runner host).
          Container port remains 5432 inside the docker network.
        required: false
        type: string
        default: "5432"

      services_csv:
        description: "CSV of service IDs (integration only)."
        required: false
        type: string
        default: ""

      scenarios_csv:
        description: "CSV of scenario IDs (integration only)."
        required: false
        type: string
        default: ""

      override_shards:
        description: >
          Optional requested shard count for integration (clamped to caps 2â€“4).
        required: false
        type: string
        default: ""

      matrix_config_path:
        description: >
          Optional path to parallel-matrix.yml (leave empty to use default in
          .github action).
        required: false
        type: string
        default: ""

      service_name:
        description: >
          Unique service identifier for artifact naming
          (e.g., python-api-example).
        required: false
        type: string
        default: ""

      # -----------------------------------------------------------------------
      # Flaky detection (PIPE-FLAKY-RERUN-INTEG-001)
      # -----------------------------------------------------------------------
      flaky_detect:
        description: "Enable suite-level flaky reruns (true|false)"
        required: false
        type: string
        default: "false"

      flaky_reruns:
        description: "Total attempts when flaky_detect=true (e.g., 3)"
        required: false
        type: string
        default: "3"

      flaky_export_path:
        description: >
          Relative path (from context/service_workdir) to store flaky evidence.
        required: false
        type: string
        default: "out/flaky"

      # -----------------------------------------------------------------------
      # Flaky policy evaluation (PIPE-FLAKY-POLICY-CONFIG-002)
      # -----------------------------------------------------------------------
      flaky_policy_path:
        description: >
          Path to policy-as-code YAML (repo root) IF repo overrides exist.
          NOTE: If missing, central policy from BrikByte-Studios/.github is used.
        required: false
        type: string
        default: ".governance/flaky-tests.yml"

      flaky_policy_enforce:
        description: >
          If true, evaluator may block ONLY if policy enables blocking.
          If false, evaluator still runs (when flaky_detect=true) but never blocks.
        required: false
        type: boolean
        default: true

    secrets:
      INTEG_DB_USER:
        required: false
      INTEG_DB_PASS:
        required: false
      INTEG_DB_NAME:
        required: false
      INTEG_DB_HOST:
        required: false
      INTEG_DB_PORT:
        required: false
      JWT_SECRET_TEST:
        required: false

permissions:
  contents: read
  packages: read

jobs:
  # ---------------------------------------------------------------------------
  # 0) PLAN: compute governed matrix for integration serviceÃ—scenario
  # ---------------------------------------------------------------------------
  plan:
    name: "Matrix Plan (integration)"
    runs-on: ubuntu-latest
    outputs:
      matrix_json: ${{ steps.matrix.outputs.matrix_json }}

    steps:
      - name: "Compute matrix plan (integration)"
        id: matrix
        uses: BrikByte-Studios/.github/.github/actions/matrix-plan@main
        with:
          test_type: "integration"
          config_path: ${{ inputs.matrix_config_path }}
          override_shards: ${{ inputs.override_shards }}
          services_csv: ${{ inputs.services_csv }}
          scenarios_csv: ${{ inputs.scenarios_csv }}

      - name: "Debug matrix_json"
        run: |
          echo "matrix_json=${{ steps.matrix.outputs.matrix_json }}"

  # ---------------------------------------------------------------------------
  # 1) RUN: containerized integration tests per shard
  # ---------------------------------------------------------------------------
  integration-tests:
    name: "Integration â€¢ shard ${{ matrix.shard }}"
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: plan

    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.plan.outputs.matrix_json) }}

    environment: integration

    env:
      SERVICE_IMAGE: ${{ inputs.image }}
      DOCKERFILE_PATH: ${{ inputs.dockerfile }}
      BUILD_CONTEXT: ${{ inputs.context }}

      DB_IMAGE: ${{ inputs.db_image }}
      CACHE_IMAGE: ${{ inputs.cache_image }}

      TEST_COMMAND: ${{ inputs.test_command }}
      TEST_LANGUAGE: ${{ inputs.test_language }}

      HEALTHCHECK_PATH: ${{ inputs.healthcheck_path }}
      HEALTHCHECK_TIMEOUT: ${{ inputs.healthcheck_timeout }}

      APP_PORT: ${{ inputs.app_port }}
      DB_HOST_PORT: ${{ inputs.db_port }}
      DB_CONTAINER_PORT: "5432"

      ENABLE_CACHE_SERVICE: ${{ inputs.enable_cache_service }}

      SERVICE_WORKDIR: ${{ inputs.context }}

      INTEG_SHARD: ${{ matrix.shard }}
      INTEG_ITEMS_CSV: ${{ matrix.items_csv }}

      INTEG_DB_USER: ${{ secrets.INTEG_DB_USER }}
      INTEG_DB_PASS: ${{ secrets.INTEG_DB_PASS }}
      INTEG_DB_NAME: ${{ secrets.INTEG_DB_NAME }}
      INTEG_DB_HOST: ${{ secrets.INTEG_DB_HOST }}
      INTEG_DB_PORT: ${{ secrets.INTEG_DB_PORT }}
      JWT_SECRET_TEST: ${{ secrets.JWT_SECRET_TEST }}

      # Flaky contract (opt-in)
      FLAKY_DETECT: ${{ inputs.flaky_detect }}
      FLAKY_RERUNS: ${{ inputs.flaky_reruns }}
      FLAKY_EXPORT_PATH: ${{ inputs.flaky_export_path }}

      # Flaky policy (PIPE-FLAKY-POLICY-CONFIG-002)
      FLAKY_POLICY_PATH: ${{ inputs.flaky_policy_path }}
      FLAKY_POLICY_ENFORCE: ${{ inputs.flaky_policy_enforce }}

      FORCE_COLOR: "0"

    steps:
      - name: "Checkout caller repository (service under test)"
        uses: actions/checkout@v4

      - name: "Checkout governance repo (.github) for shared templates"
        uses: actions/checkout@v4
        with:
          repository: BrikByte-Studios/.github
          path: org-dotgithub

      - name: "Sync shared integration-test templates into caller repo"
        run: |
          set -euo pipefail
          mkdir -p .github/scripts .github/templates

          cp org-dotgithub/.github/scripts/run-integration-tests.sh \
            .github/scripts/run-integration-tests.sh
          cp org-dotgithub/.github/scripts/db-load-fixtures.sh \
            .github/scripts/db-load-fixtures.sh
          cp org-dotgithub/.github/scripts/db-seed-json.mjs \
            .github/scripts/db-seed-json.mjs
          cp org-dotgithub/.github/scripts/wait-for-health.sh \
            .github/scripts/wait-for-health.sh
          cp org-dotgithub/.github/scripts/export-integration-audit.mjs \
            .github/scripts/export-integration-audit.mjs
          cp org-dotgithub/.github/scripts/env-generate-integration.sh \
            .github/scripts/env-generate-integration.sh

          cp org-dotgithub/.github/scripts/flaky-rerun.sh \
            .github/scripts/flaky-rerun.sh

          # Evaluator (PIPE-FLAKY-POLICY-CONFIG-002)
          cp org-dotgithub/.github/scripts/evaluate-flaky.ts .github/scripts/evaluate-flaky.ts
          cp org-dotgithub/.github/scripts/flaky-io.ts       .github/scripts/flaky-io.ts
          cp org-dotgithub/.github/scripts/flaky-types.ts    .github/scripts/flaky-types.ts || true
          cp org-dotgithub/.github/scripts/export-flaky-analytics.mjs \
            .github/scripts/export-flaky-analytics.mjs

          cp org-dotgithub/.github/templates/integration-test-runner.Dockerfile \
            .github/templates/integration-test-runner.Dockerfile

          chmod +x .github/scripts/run-integration-tests.sh
          chmod +x .github/scripts/db-load-fixtures.sh
          chmod +x .github/scripts/wait-for-health.sh
          chmod +x .github/scripts/env-generate-integration.sh
          chmod +x .github/scripts/flaky-rerun.sh

          echo "Shared integration-test templates synced."
          ls -R .github

      - name: "Parallel runner (contract only)"
        id: pr
        uses: BrikByte-Studios/.github/.github/actions/parallel-runner@main
        with:
          parallel_enabled: "true"
          parallel_mode: "static"
          shard_index: ${{ matrix.shard }}
          shard_total: ${{ strategy.job-total }}
          selection_mode: "none"
          working_directory: ${{ env.SERVICE_WORKDIR }}

      - name: "Log integration-test configuration"
        run: |
          set -euo pipefail
          echo "Service image        : ${SERVICE_IMAGE}"
          echo "Dockerfile path      : ${DOCKERFILE_PATH}"
          echo "Build context        : ${BUILD_CONTEXT}"
          echo "DB image             : ${DB_IMAGE}"
          echo "Cache image          : ${CACHE_IMAGE}"
          echo "Test command         : ${TEST_COMMAND}"
          echo "Test language        : ${TEST_LANGUAGE}"
          echo "Health path          : ${HEALTHCHECK_PATH}"
          echo "Health timeout       : ${HEALTHCHECK_TIMEOUT}s"
          echo "App port             : ${APP_PORT}"
          echo "DB container port    : ${DB_CONTAINER_PORT}"
          echo "DB host port         : ${DB_HOST_PORT}"
          echo "Enable cache service : ${ENABLE_CACHE_SERVICE}"
          echo "Shard                : ${INTEG_SHARD}/${{ strategy.job-total }}"
          echo "Items                : ${INTEG_ITEMS_CSV}"
          echo "FlakyDetect          : ${FLAKY_DETECT}"
          echo "FlakyReruns          : ${FLAKY_RERUNS}"
          echo "FlakyExport          : ${FLAKY_EXPORT_PATH}"
          echo "FlakyEnforce         : ${FLAKY_POLICY_ENFORCE}"
          echo "FlakyPolicyPath      : ${FLAKY_POLICY_PATH}"

      - name: "Login to GHCR (if using GitHub Container Registry)"
        if: startsWith(env.SERVICE_IMAGE, 'ghcr.io/')
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: "Ensure service image exists (pull or build)"
        run: |
          set -euo pipefail
          echo "Attempting to pull service image: ${SERVICE_IMAGE}"
          if docker pull "${SERVICE_IMAGE}"; then
            echo "Pulled existing service image."
          else
            echo "WARNING: pull failed; building locally..."
            docker build \
              -f "${DOCKERFILE_PATH}" \
              -t "${SERVICE_IMAGE}" \
              "${BUILD_CONTEXT}"
          fi

      - name: "Build integration test runner image"
        run: |
          set -euo pipefail
          docker build \
            -f .github/templates/integration-test-runner.Dockerfile \
            --build-arg SERVICE_WORKDIR="${SERVICE_WORKDIR}" \
            -t brikpipe/integration-test-runner:latest \
            .

      - name: "Create dedicated Docker network"
        run: |
          set -euo pipefail
          NETWORK_NAME="brikpipe-integ-net"
          docker network inspect "${NETWORK_NAME}" >/dev/null 2>&1 || \
            docker network create "${NETWORK_NAME}"
          echo "NETWORK_NAME=${NETWORK_NAME}" >> "$GITHUB_ENV"
          echo "Using network: ${NETWORK_NAME}"

      - name: "Start DB container"
        run: |
          set -euo pipefail
          echo "Starting DB container: ${DB_IMAGE}"
          docker run -d \
            --name db \
            --network "${NETWORK_NAME}" \
            -e POSTGRES_USER=testuser \
            -e POSTGRES_PASSWORD=testpass \
            -e POSTGRES_DB=testdb \
            -p "${DB_HOST_PORT}:${DB_CONTAINER_PORT}" \
            --cpus="1.0" \
            --memory="512m" \
            "${DB_IMAGE}"
          docker ps

      - name: "Install JSON seeding dependencies (pg)"
        run: |
          set -euo pipefail
          cd .github/scripts
          if [ ! -f package.json ]; then
            npm init -y >/dev/null 2>&1
          fi
          npm install pg

      - name: "Load DB fixtures for integration tests"
        env:
          ENABLE_DB_FIXTURES: "true"
          DB_ENGINE: "postgres"
          DB_HOST: "localhost"
          DB_PORT: "${{ env.DB_HOST_PORT }}"
          DB_USER: "testuser"
          DB_PASSWORD: "testpass"
          DB_NAME: "testdb"
          FIXTURE_DIR: "${{ env.BUILD_CONTEXT }}/tests/integration/fixtures/db"
        run: |
          set -euo pipefail
          ./.github/scripts/db-load-fixtures.sh

      - name: "Generate integration runtime env file"
        run: |
          set -euo pipefail
          SERVICE_WORKDIR="${BUILD_CONTEXT}" .github/scripts/env-generate-integration.sh
          ls -la "${BUILD_CONTEXT}"

      - name: "Start cache/message broker container (optional)"
        if: env.ENABLE_CACHE_SERVICE == 'true'
        run: |
          set -euo pipefail
          if [ -z "${CACHE_IMAGE}" ]; then
            echo "::error::ENABLE_CACHE_SERVICE is true but CACHE_IMAGE is empty."
            exit 1
          fi
          docker run -d \
            --name cache \
            --network "${NETWORK_NAME}" \
            --cpus="0.5" \
            --memory="256m" \
            "${CACHE_IMAGE}"
          docker ps

      - name: "Start app container"
        run: |
          set -euo pipefail

          RUNTIME_ENV_FILE="${BUILD_CONTEXT}/.env.integ.runtime"
          if [ ! -f "${RUNTIME_ENV_FILE}" ]; then
            echo "::error::Expected runtime env file not found: ${RUNTIME_ENV_FILE}"
            exit 1
          fi

          docker run -d \
            --name app \
            --network "${NETWORK_NAME}" \
            --env-file "${RUNTIME_ENV_FILE}" \
            -e DB_HOST=db \
            -e DB_PORT="${DB_CONTAINER_PORT}" \
            -e CACHE_HOST=cache \
            -e CACHE_PORT=6379 \
            -e EXTERNAL_API_STUB_MODE=true \
            -p "${APP_PORT}:${APP_PORT}" \
            --cpus="1.0" \
            --memory="512m" \
            "${SERVICE_IMAGE}"

          docker ps

      - name: "Run integration tests in runner container (flaky opt-in)"
        run: |
          set -euo pipefail

          APP_BASE_URL="http://app:${APP_PORT}"
          APP_HEALTH_URL="${APP_BASE_URL}${HEALTHCHECK_PATH}"

          INTEG_JOB_STARTED_AT="$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "INTEG_JOB_STARTED_AT=${INTEG_JOB_STARTED_AT}" >> "$GITHUB_ENV"

          export FLAKY_DETECT="${FLAKY_DETECT:-false}"
          export FLAKY_RERUNS="${FLAKY_RERUNS:-3}"
          export FLAKY_EXPORT_PATH="${PWD}/${SERVICE_WORKDIR}/${FLAKY_EXPORT_PATH:-out/flaky}"
          export FLAKY_LABEL="integration-${TEST_LANGUAGE:-generic}"

          FLAKY_SH="${GITHUB_WORKSPACE}/.github/scripts/flaky-rerun.sh"
          if [[ ! -f "${FLAKY_SH}" ]]; then
            echo "âŒ flaky-rerun.sh not found at: ${FLAKY_SH}"
            ls -la "${GITHUB_WORKSPACE}/.github/scripts" || true
            exit 1
          fi
          chmod +x "${FLAKY_SH}"

          "${FLAKY_SH}" -- docker run --rm \
            --name tests \
            --network "${NETWORK_NAME}" \
            -v "${PWD}/${SERVICE_WORKDIR}:/workspace/${SERVICE_WORKDIR}" \
            -e APP_BASE_URL="${APP_BASE_URL}" \
            -e APP_HEALTH_URL="${APP_HEALTH_URL}" \
            -e DB_HOST="db" \
            -e DB_PORT="${DB_CONTAINER_PORT}" \
            -e HEALTHCHECK_TIMEOUT="${HEALTHCHECK_TIMEOUT}" \
            -e TEST_LANGUAGE="${TEST_LANGUAGE}" \
            -e TEST_COMMAND="${TEST_COMMAND}" \
            -e SERVICE_WORKDIR="${SERVICE_WORKDIR}" \
            -e INTEG_SHARD="${INTEG_SHARD}" \
            -e INTEG_ITEMS_CSV="${INTEG_ITEMS_CSV}" \
            brikpipe/integration-test-runner:latest

      - name: "Dump service logs on failure (diagnostic aid)"
        if: failure()
        run: |
          set -euo pipefail
          echo "==== Docker logs: db ===="
          docker logs db || true
          echo "==== Docker logs: cache ===="
          docker logs cache || true
          echo "==== Docker logs: app ===="
          docker logs app || true

      - name: "Capture integration container logs to workspace"
        if: always()
        run: |
          set -euo pipefail
          LOG_DIR="${SERVICE_WORKDIR}/out/integration-logs"
          mkdir -p "${LOG_DIR}"

          docker logs app > "${LOG_DIR}/app.log" 2>&1 || true
          docker logs db  > "${LOG_DIR}/db.log" 2>&1 || true

          if docker ps -a --format '{{.Names}}' | grep -q '^cache$'; then
            docker logs cache > "${LOG_DIR}/cache.log" 2>&1 || true
          fi

          echo "Captured logs:"
          ls -la "${LOG_DIR}"

      - name: "Tear down containers and network"
        if: always()
        run: |
          set -euo pipefail
          docker rm -f tests app db cache 2>/dev/null || true
          docker network rm "${NETWORK_NAME}" 2>/dev/null || true

      # =============================================================================
      # PIPE-FLAKY-POLICY-CONFIG-002 â€” Evaluate evidence against policy-as-code
      # =============================================================================

      - name: "Setup Node (for flaky evaluator)"
        if: ${{ always() && env.FLAKY_DETECT == 'true' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: "Select policy source (repo override -> central fallback)"
        if: ${{ always() && env.FLAKY_DETECT == 'true' }}
        run: |
          set -euo pipefail

          mkdir -p .tmp-flaky-eval

          REPO_POLICY="${GITHUB_WORKSPACE}/${FLAKY_POLICY_PATH}"
          CENTRAL_POLICY="${GITHUB_WORKSPACE}/org-dotgithub/.governance/flaky-tests.yml"
          OUT_POLICY="${GITHUB_WORKSPACE}/.tmp-flaky-eval/policy.yml"

          echo "ðŸ§© [FLAKY] Repo policy     : ${REPO_POLICY}"
          echo "ðŸ›ï¸ [FLAKY] Central policy  : ${CENTRAL_POLICY}"
          echo "ðŸ“Œ [FLAKY] Selected policy : ${OUT_POLICY}"

          if [[ -f "${REPO_POLICY}" ]]; then
            echo "âœ… [FLAKY] Using repo override policy."
            cp "${REPO_POLICY}" "${OUT_POLICY}"
          else
            if [[ ! -f "${CENTRAL_POLICY}" ]]; then
              echo "âŒ [FLAKY] Central policy missing at: ${CENTRAL_POLICY}"
              exit 1
            fi
            echo "âœ… [FLAKY] Repo policy not found; using central policy."
            cp "${CENTRAL_POLICY}" "${OUT_POLICY}"
          fi

      - name: "Install evaluator deps (repo-root, pinned) [js-yaml + tsx]"
        if: ${{ always() && env.FLAKY_DETECT == 'true' }}
        working-directory: ${{ github.workspace }}
        env:
          NPM_CONFIG_FUND: "false"
          NPM_CONFIG_AUDIT: "false"
        run: |
          set -euo pipefail
          node -v
          npm -v

          if [[ ! -f package.json ]]; then
            npm init -y >/dev/null 2>&1
          fi

          npm i --no-save js-yaml@4 tsx@4 >/dev/null 2>&1
          echo "âœ… deps installed into repo-root node_modules"

      - name: "Normalize flaky evidence â†’ summary.normalized.json"
        if: ${{ always() && env.FLAKY_DETECT == 'true' }}
        run: |
          set -euo pipefail

          EXPORT_DIR="${GITHUB_WORKSPACE}/${SERVICE_WORKDIR}/${FLAKY_EXPORT_PATH}"
          EXPECTED="${EXPORT_DIR}/summary.json"
          LABELED="${EXPORT_DIR}/${FLAKY_LABEL:-integration-generic}.summary.json"

          EVIDENCE=""
          if [[ -f "${EXPECTED}" ]]; then
            EVIDENCE="${EXPECTED}"
          elif [[ -f "${LABELED}" ]]; then
            EVIDENCE="${LABELED}"
          else
            found="$(ls -1 "${EXPORT_DIR}"/*.summary.json 2>/dev/null | head -n 1 || true)"
            if [[ -n "${found}" ]]; then EVIDENCE="${found}"; fi
          fi

          if [[ -z "${EVIDENCE}" ]]; then
            echo "â„¹ï¸ [FLAKY] No evidence found â†’ skipping normalization."
            ls -la "${EXPORT_DIR}" || true
            exit 0
          fi

          OUT_NORM="${EXPORT_DIR}/summary.normalized.json"
          echo "ðŸ”§ [FLAKY] Normalizing: ${EVIDENCE} -> ${OUT_NORM}"

          EVIDENCE="${EVIDENCE}" OUT_NORM="${OUT_NORM}" python - <<'PY'
          import json
          from pathlib import Path
          import os

          evidence = Path(os.environ["EVIDENCE"])
          out_norm = Path(os.environ["OUT_NORM"])

          data = json.loads(evidence.read_text(encoding="utf-8"))

          attempts = data.get("attempts") if isinstance(data, dict) else None
          pass_count = 0
          fail_count = 0
          total_attempts = 0

          if isinstance(attempts, list) and attempts:
            total_attempts = len(attempts)
            for a in attempts:
              s = (a.get("status") or "").strip().lower()
              if not s:
                rc = int(a.get("exit_code", 0) or 0)
                s = "pass" if rc == 0 else "fail"
                a["status"] = s
              if s in ("pass", "passed", "ok", "success"):
                pass_count += 1
              elif s in ("fail", "failed", "error"):
                fail_count += 1
          else:
            pass_count = int(data.get("pass_count", 0) or 0) if isinstance(data, dict) else 0
            fail_count = int(data.get("fail_count", 0) or 0) if isinstance(data, dict) else 0
            total_attempts = pass_count + fail_count

          if isinstance(data, dict):
            data["pass_count"] = pass_count
            data["fail_count"] = fail_count
            data["total_attempts"] = total_attempts
            data["attempts"] = attempts if isinstance(attempts, list) else data.get("attempts", [])

          out_norm.parent.mkdir(parents=True, exist_ok=True)
          out_norm.write_text(json.dumps(data, indent=2), encoding="utf-8")
          print(f"âœ… wrote {out_norm} (pass={pass_count}, fail={fail_count}, total={total_attempts})")
          PY

      - name: "Evaluate flaky policy (use normalized summary)"
        if: ${{ always() && env.FLAKY_DETECT == 'true' }}
        run: |
          set -euo pipefail

          POLICY="${GITHUB_WORKSPACE}/.tmp-flaky-eval/policy.yml"
          EXPORT_DIR="${GITHUB_WORKSPACE}/${SERVICE_WORKDIR}/${FLAKY_EXPORT_PATH}"
          IN_NORM="${EXPORT_DIR}/summary.normalized.json"
          OUT_JSON="${EXPORT_DIR}/evaluation.json"

          echo "ðŸ§© [FLAKY] Policy     : ${POLICY}"
          echo "ðŸ“ [FLAKY] Export dir : ${EXPORT_DIR}"
          echo "ðŸ“¥ [FLAKY] Normalized : ${IN_NORM}"
          echo "ðŸ“¤ [FLAKY] Output     : ${OUT_JSON}"
          echo "ðŸ›¡ï¸ [FLAKY] Enforce    : ${FLAKY_POLICY_ENFORCE}"

          if [[ ! -f "${POLICY}" ]]; then
            echo "âŒ [FLAKY] Missing policy file: ${POLICY}"
            exit 1
          fi

          if [[ ! -f "${IN_NORM}" ]]; then
            echo "â„¹ï¸ [FLAKY] Normalized summary missing â†’ skipping evaluation (non-blocking)."
            ls -la "${EXPORT_DIR}" || true
            exit 0
          fi

          EVAL_TS="${GITHUB_WORKSPACE}/.github/scripts/evaluate-flaky.ts"
          if [[ ! -f "${EVAL_TS}" ]]; then
            echo "âŒ [FLAKY] evaluate-flaky.ts not found at: ${EVAL_TS}"
            ls -la "${GITHUB_WORKSPACE}/.github/scripts" || true
            exit 1
          fi

          pushd "${GITHUB_WORKSPACE}" >/dev/null
          npx tsx "${EVAL_TS}" \
            --policy "${POLICY}" \
            --input "${IN_NORM}" \
            --out "${OUT_JSON}" \
            --md || rc=$?
          popd >/dev/null

          if [[ "${rc:-0}" != "0" && "${FLAKY_POLICY_ENFORCE}" != "true" ]]; then
            echo "âš ï¸ [FLAKY] Evaluator exit=${rc} but enforce=false â†’ not blocking."
            exit 0
          fi

          exit "${rc:-0}"

      - name: "Export flaky analytics (when enabled)"
        if: ${{ always() && env.FLAKY_DETECT == 'true' }}
        working-directory: ${{ env.SERVICE_WORKDIR }}
        run: |
          set -euo pipefail
          FLAKY_JS="${GITHUB_WORKSPACE}/.github/scripts/export-flaky-analytics.mjs"

          if [[ ! -f "${FLAKY_JS}" ]]; then
            echo "âŒ export-flaky-analytics.mjs not found at: ${FLAKY_JS}"
            ls -la "${GITHUB_WORKSPACE}/.github/scripts" || true
            exit 1
          fi

          chmod +x "${FLAKY_JS}"
          node "${FLAKY_JS}" \
            --suite integration \
            --audit-root "${GITHUB_WORKSPACE}/.audit" \
            --out-dir "${FLAKY_EXPORT_PATH:-out/flaky}" \
            --top-n 10 \
            --write-md true \
            --allow-missing-evaluation true

      - name: "Upload flaky evaluation artifact (always)"
        if: ${{ always() && env.FLAKY_DETECT == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: flaky-eval-integration-${{ inputs.service_name != '' && inputs.service_name || github.workflow }}-shard${{ matrix.shard }}
          path: |
            ${{ inputs.context }}/${{ inputs.flaky_export_path }}/
          if-no-files-found: warn
          retention-days: 14
          include-hidden-files: true

      - name: "Ensure .audit root exists (always)"
        if: always()
        run: |
          set -euo pipefail
          mkdir -p .audit
          echo "Audit root ready: $(pwd)/.audit"

      - name: "Export integration audit bundle"
        if: always()
        run: |
          set -euo pipefail

          mkdir -p .audit

          JUNIT_PATH="${SERVICE_WORKDIR}/out/junit-integration.xml"
          RESULTS_PATH="${SERVICE_WORKDIR}/out/integration-results.json"
          LOGS_DIR="${SERVICE_WORKDIR}/out/integration-logs"
          COVERAGE_PATH="${SERVICE_WORKDIR}/out/coverage-integration.json"

          echo "Exporting integration audit bundle..."
          echo "JUNIT_PATH=${JUNIT_PATH}"
          echo "RESULTS_PATH=${RESULTS_PATH}"
          echo "LOGS_DIR=${LOGS_DIR}"
          echo "COVERAGE_PATH=${COVERAGE_PATH}"

          node .github/scripts/export-integration-audit.mjs \
            --junit "${JUNIT_PATH}" \
            --results "${RESULTS_PATH}" \
            --logs-dir "${LOGS_DIR}" \
            --coverage "${COVERAGE_PATH}"

          echo "==== .audit tree ===="
          find .audit -maxdepth 6 -type f -print || true

      - name: "Upload integration audit artifact (always)"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: >-
            integration-audit-
            ${{ inputs.service_name != '' && inputs.service_name || github.workflow }}-
            ${{ github.run_id }}-
            shard-${{ matrix.shard }}-
            attempt-${{ github.run_attempt }}
          path: .audit
          if-no-files-found: warn
          retention-days: 14
          include-hidden-files: true
